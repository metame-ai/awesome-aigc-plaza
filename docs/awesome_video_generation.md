# Awesome Video Generation

- [Awesome Video Generation](#awesome-video-generation)
	- [Survey](#survey)
	- [Video Generation](#video-generation)
	- [Animation](#animation)
	- [Video Editting](#video-editting)
	- [Datasets](#datasets)
	- [Toolkits](#toolkits)
	- [Projects](#projects)
	- [Other](#other)
	- [Product](#product)

## Survey
- **A Comprehensive Survey on Human Video Generation: Challenges, Methods,
  and Insights**, `arXiv, 2407.08428`, [arxiv](http://arxiv.org/abs/2407.08428v1), [pdf](http://arxiv.org/pdf/2407.08428v1.pdf), cication: [**-1**](None)

	 *Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, Li Liu*
- **From Sora What We Can See: A Survey of Text-to-Video Generation**, `arXiv, 2405.10674`, [arxiv](http://arxiv.org/abs/2405.10674v1), [pdf](http://arxiv.org/pdf/2405.10674v1.pdf), cication: [**-1**](None)

	 *Rui Sun, Yumin Zhang, Tejal Shah, Jiahao Sun, Shuoying Zhang, Wenqi Li, Haoran Duan, Bo Wei, Rajiv Ranjan* · ([awesome-text-to-video-generation](https://github.com/soraw-ai/awesome-text-to-video-generation) - soraw-ai) ![Star](https://img.shields.io/github/stars/soraw-ai/awesome-text-to-video-generation.svg?style=social&label=Star)
- **Is Sora a World Simulator? A Comprehensive Survey on General World
  Models and Beyond**, `arXiv, 2405.03520`, [arxiv](http://arxiv.org/abs/2405.03520v1), [pdf](http://arxiv.org/pdf/2405.03520v1.pdf), cication: [**-1**](None)

	 *Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang* · ([General-World-Models-Survey](https://github.com/GigaAI-research/General-World-Models-Survey) - GigaAI-research) ![Star](https://img.shields.io/github/stars/GigaAI-research/General-World-Models-Survey.svg?style=social&label=Star)
- **Video Diffusion Models: A Survey**, `arXiv, 2405.03150`, [arxiv](http://arxiv.org/abs/2405.03150v1), [pdf](http://arxiv.org/pdf/2405.03150v1.pdf), cication: [**-1**](None)

	 *Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, Helge Ritter*
- **A Survey on Long Video Generation: Challenges, Methods, and Prospects**, `arXiv, 2403.16407`, [arxiv](http://arxiv.org/abs/2403.16407v1), [pdf](http://arxiv.org/pdf/2403.16407v1.pdf), cication: [**-1**](None)

	 *Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, Lei Bai*
- **Sora as an AGI World Model? A Complete Survey on Text-to-Video
  Generation**, `arXiv, 2403.05131`, [arxiv](http://arxiv.org/abs/2403.05131v1), [pdf](http://arxiv.org/pdf/2403.05131v1.pdf), cication: [**-1**](None)

	 *Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang*
- [**sorareview**](https://github.com/lichao-sun/sorareview) - lichao-sun ![Star](https://img.shields.io/github/stars/lichao-sun/sorareview.svg?style=social&label=Star)

	 *The official GitHub page for the review paper "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models".*
- **A Survey on Generative Diffusion Model**, `ieee transactions on knowledge and data engineering, 2024`, [arxiv](http://arxiv.org/abs/2209.02646v10), [pdf](http://arxiv.org/pdf/2209.02646v10.pdf), cication: [**121**](https://scholar.google.com/scholar?cites=16128994104714577384&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li* · ([A-Survey-on-Generative-Diffusion-Model](https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model) - chq1155) ![Star](https://img.shields.io/github/stars/chq1155/A-Survey-on-Generative-Diffusion-Model.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-22-2))

## Video Generation
- **VEnhancer: Generative Space-Time Enhancement for Video Generation**, `arXiv, 2407.07667`, [arxiv](http://arxiv.org/abs/2407.07667v1), [pdf](http://arxiv.org/pdf/2407.07667v1.pdf), cication: [**-1**](None)

	 *Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, Ziwei Liu*
- **Controlling Space and Time with Diffusion Models**, `arXiv, 2407.07860`, [arxiv](http://arxiv.org/abs/2407.07860v1), [pdf](http://arxiv.org/pdf/2407.07860v1.pdf), cication: [**-1**](None)

	 *Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, David J. Fleet*
- **Compositional Video Generation as Flow Equalization**, `arXiv, 2407.06182`, [arxiv](http://arxiv.org/abs/2407.06182v1), [pdf](http://arxiv.org/pdf/2407.06182v1.pdf), cication: [**-1**](None)

	 *Xingyi Yang, Xinchao Wang*

	 · ([adamdad.github](https://adamdad.github.io/vico/))
- **Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT**, `arXiv, 2406.18583`, [arxiv](http://arxiv.org/abs/2406.18583v1), [pdf](http://arxiv.org/pdf/2406.18583v1.pdf), cication: [**-1**](None)

	 *Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma* · ([Lumina-T2X](https://github.com/Alpha-VLLM/Lumina-T2X) - Alpha-VLLM) ![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-T2X.svg?style=social&label=Star)
- **MotionBooth: Motion-Aware Customized Text-to-Video Generation**, `arXiv, 2406.17758`, [arxiv](http://arxiv.org/abs/2406.17758v1), [pdf](http://arxiv.org/pdf/2406.17758v1.pdf), cication: [**-1**](None)

	 *Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen* · ([jianzongwu.github](https://jianzongwu.github.io/projects/motionbooth/))
- [**OpenDiT**](https://github.com/NUS-HPC-AI-Lab/OpenDiT?tab=readme-ov-file#pyramid-attention-broadcast-pab-blogdoc) - NUS-HPC-AI-Lab ![Star](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/OpenDiT.svg?style=social&label=Star)

	 *OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference* · ([oahzxl.github](https://oahzxl.github.io/PAB/))
- **Text-Animator: Controllable Visual Text Video Generation**, `arXiv, 2406.17777`, [arxiv](http://arxiv.org/abs/2406.17777v1), [pdf](http://arxiv.org/pdf/2406.17777v1.pdf), cication: [**-1**](None)

	 *Lin Liu, Quande Liu, Shengju Qian, Yuan Zhou, Wengang Zhou, Houqiang Li, Lingxi Xie, Qi Tian* · ([laulampaul.github](https://laulampaul.github.io/text-animator.html))
- **Video-Infinity: Distributed Long Video Generation**, `arXiv, 2406.16260`, [arxiv](http://arxiv.org/abs/2406.16260v1), [pdf](http://arxiv.org/pdf/2406.16260v1.pdf), cication: [**-1**](None)

	 *Zhenxiong Tan, Xingyi Yang, Songhua Liu, Xinchao Wang* · ([Video-Infinity](https://github.com/Yuanshi9815/Video-Infinity) - Yuanshi9815) ![Star](https://img.shields.io/github/stars/Yuanshi9815/Video-Infinity.svg?style=social&label=Star) · ([video-infinity.tanzhenxiong](https://video-infinity.tanzhenxiong.com/))
- **ExVideo: Extending Video Diffusion Models via Parameter-Efficient
  Post-Tuning**, `arXiv, 2406.14130`, [arxiv](http://arxiv.org/abs/2406.14130v1), [pdf](http://arxiv.org/pdf/2406.14130v1.pdf), cication: [**-1**](None)

	 *Zhongjie Duan, Wenmeng Zhou, Cen Chen, Yaliang Li, Weining Qian* · ([ecnu-cilab.github](https://ecnu-cilab.github.io/ExVideoProjectPage/))
- **Training-free Camera Control for Video Generation**, `arXiv, 2406.10126`, [arxiv](http://arxiv.org/abs/2406.10126v1), [pdf](http://arxiv.org/pdf/2406.10126v1.pdf), cication: [**-1**](None)

	 *Chen Hou, Guoqiang Wei, Yan Zeng, Zhibo Chen* · ([lifedecoder.github](https://lifedecoder.github.io/CamTrol/))
- [LAMP: Learn A Motion Pattern for Few-Shot Video Generation](https://rq-wu.github.io/projects/LAMP/)

	 · ([LAMP](https://github.com/RQ-Wu/LAMP) - RQ-Wu) ![Star](https://img.shields.io/github/stars/RQ-Wu/LAMP.svg?style=social&label=Star)
- **Hierarchical Patch Diffusion Models for High-Resolution Video Generation**, `arXiv, 2406.07792`, [arxiv](http://arxiv.org/abs/2406.07792v1), [pdf](http://arxiv.org/pdf/2406.07792v1.pdf), cication: [**-1**](None)

	 *Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov* · ([snap-research.github](https://snap-research.github.io/hpdm))
- **MotionClone: Training-Free Motion Cloning for Controllable Video
  Generation**, `arXiv, 2406.05338`, [arxiv](http://arxiv.org/abs/2406.05338v2), [pdf](http://arxiv.org/pdf/2406.05338v2.pdf), cication: [**-1**](None)

	 *Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, Yi Jin* · ([MotionClone](https://github.com/Bujiazi/MotionClone/) - Bujiazi) ![Star](https://img.shields.io/github/stars/Bujiazi/MotionClone.svg?style=social&label=Star)
- **CV-VAE: A Compatible Video VAE for Latent Generative Video Models**, `arXiv, 2405.20279`, [arxiv](http://arxiv.org/abs/2405.20279v1), [pdf](http://arxiv.org/pdf/2405.20279v1.pdf), cication: [**-1**](None)

	 *Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan* · ([cv-vae](https://github.com/ailab-cvc/cv-vae) - ailab-cvc) ![Star](https://img.shields.io/github/stars/ailab-cvc/cv-vae.svg?style=social&label=Star)
- **VideoTetris: Towards Compositional Text-to-Video Generation**, `arXiv, 2406.04277`, [arxiv](http://arxiv.org/abs/2406.04277v1), [pdf](http://arxiv.org/pdf/2406.04277v1.pdf), cication: [**-1**](None)

	 *Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan*

	 · ([VideoTetris](https://github.com/YangLing0818/VideoTetris) - YangLing0818) ![Star](https://img.shields.io/github/stars/YangLing0818/VideoTetris.svg?style=social&label=Star)
- **Searching Priors Makes Text-to-Video Synthesis Better**, `arXiv, 2406.03215`, [arxiv](http://arxiv.org/abs/2406.03215v1), [pdf](http://arxiv.org/pdf/2406.03215v1.pdf), cication: [**-1**](None)

	 *Haoran Cheng, Liang Peng, Linxuan Xia, Yuepeng Hu, Hengjia Li, Qinglin Lu, Xiaofei He, Boxi Wu*
- **SF-V: Single Forward Video Generation Model**, `arXiv, 2406.04324`, [arxiv](http://arxiv.org/abs/2406.04324v1), [pdf](http://arxiv.org/pdf/2406.04324v1.pdf), cication: [**-1**](None)

	 *Zhixing Zhang, Yanyu Li, Yushu Wu, Yanwu Xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas*

	 · ([snap-research.github](https://snap-research.github.io/SF-V/))
- **ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video
  Generation**, `arXiv, 2406.00908`, [arxiv](http://arxiv.org/abs/2406.00908v1), [pdf](http://arxiv.org/pdf/2406.00908v1.pdf), cication: [**-1**](None)

	 *Shaoshu Yang, Yong Zhang, Xiaodong Cun, Ying Shan, Ran He*
- **T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model
  with Mixed Reward Feedback**, `arXiv, 2405.18750`, [arxiv](http://arxiv.org/abs/2405.18750v1), [pdf](http://arxiv.org/pdf/2405.18750v1.pdf), cication: [**-1**](None)

	 *Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, William Yang Wang*
- **Collaborative Video Diffusion: Consistent Multi-video Generation with
  Camera Control**, `arXiv, 2405.17414`, [arxiv](http://arxiv.org/abs/2405.17414v1), [pdf](http://arxiv.org/pdf/2405.17414v1.pdf), cication: [**-1**](None)

	 *Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, Gordon Wetzstein* · ([collaborativevideodiffusion.github](https://collaborativevideodiffusion.github.io/.))
- **FIFO-Diffusion: Generating Infinite Videos from Text without Training**, `arXiv, 2405.11473`, [arxiv](http://arxiv.org/abs/2405.11473v1), [pdf](http://arxiv.org/pdf/2405.11473v1.pdf), cication: [**-1**](None)

	 *Jihwan Kim, Junoh Kang, Jinyoung Choi, Bohyung Han*
- **Lumina-T2X: Transforming Text into Any Modality, Resolution, and
  Duration via Flow-based Large Diffusion Transformers**, `arXiv, 2405.05945`, [arxiv](http://arxiv.org/abs/2405.05945v1), [pdf](http://arxiv.org/pdf/2405.05945v1.pdf), cication: [**-1**](None)

	 *Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang* · ([Lumina-T2X](https://github.com/Alpha-VLLM/Lumina-T2X?tab=readme-ov-file) - Alpha-VLLM) ![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-T2X.svg?style=social&label=Star)
- **Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator
  with Diffusion Models**, `arXiv, 2405.04233`, [arxiv](http://arxiv.org/abs/2405.04233v1), [pdf](http://arxiv.org/pdf/2405.04233v1.pdf), cication: [**-1**](None)

	 *Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, Jun Zhu* · ([shengshu-ai](https://www.shengshu-ai.com/vidu))
- **StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video
  Generation**, `arXiv, 2405.01434`, [arxiv](http://arxiv.org/abs/2405.01434v1), [pdf](http://arxiv.org/pdf/2405.01434v1.pdf), cication: [**-1**](None)

	 *Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, Qibin Hou* · ([storydiffusion](https://github.com/hvision-nku/storydiffusion) - hvision-nku) ![Star](https://img.shields.io/github/stars/hvision-nku/storydiffusion.svg?style=social&label=Star) · ([storydiffusion.github](https://storydiffusion.github.io/))
- **MotionMaster: Training-free Camera Motion Transfer For Video Generation**, `arXiv, 2404.15789`, [arxiv](http://arxiv.org/abs/2404.15789v1), [pdf](http://arxiv.org/pdf/2404.15789v1.pdf), cication: [**-1**](None)

	 *Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, Lizhuang Ma*

	 · ([sjtuplayer.github](https://sjtuplayer.github.io/projects/MotionMaster/)) · ([MotionMaster](https://github.com/sjtuplayer/MotionMaster) - sjtuplayer) ![Star](https://img.shields.io/github/stars/sjtuplayer/MotionMaster.svg?style=social&label=Star)
- **MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators**, `arXiv, 2404.05014`, [arxiv](http://arxiv.org/abs/2404.05014v1), [pdf](http://arxiv.org/pdf/2404.05014v1.pdf), cication: [**-1**](None)

	 *Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, Jiebo Luo*
- **CameraCtrl: Enabling Camera Control for Text-to-Video Generation**, `arXiv, 2404.02101`, [arxiv](http://arxiv.org/abs/2404.02101v1), [pdf](http://arxiv.org/pdf/2404.02101v1.pdf), cication: [**-1**](None)

	 *Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, Ceyuan Yang* · ([hehao13.github](https://hehao13.github.io/projects-CameraCtrl))
- **Grid Diffusion Models for Text-to-Video Generation**, `arXiv, 2404.00234`, [arxiv](http://arxiv.org/abs/2404.00234v1), [pdf](http://arxiv.org/pdf/2404.00234v1.pdf), cication: [**-1**](None)

	 *Taegyeong Lee, Soyeong Kwon, Taehwan Kim*
- **StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation
  from Text**, `arXiv, 2403.14773`, [arxiv](http://arxiv.org/abs/2403.14773v1), [pdf](http://arxiv.org/pdf/2403.14773v1.pdf), cication: [**-1**](None)

	 *Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi* · ([StreamingT2V](https://github.com/Picsart-AI-Research/StreamingT2V) - Picsart-AI-Research) ![Star](https://img.shields.io/github/stars/Picsart-AI-Research/StreamingT2V.svg?style=social&label=Star)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652460909&idx=5&sn=495c83341f50bec67316855f61d6bdd8))
- **Efficient Video Diffusion Models via Content-Frame Motion-Latent
  Decomposition**, `arXiv, 2403.14148`, [arxiv](http://arxiv.org/abs/2403.14148v1), [pdf](http://arxiv.org/pdf/2403.14148v1.pdf), cication: [**-1**](None)

	 *Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar*
- **Mora: Enabling Generalist Video Generation via A Multi-Agent Framework**, `arXiv, 2403.13248`, [arxiv](http://arxiv.org/abs/2403.13248v1), [pdf](http://arxiv.org/pdf/2403.13248v1.pdf), cication: [**-1**](None)

	 *Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, Lichao Sun* · ([Mora](https://github.com/lichao-sun/Mora) - lichao-sun) ![Star](https://img.shields.io/github/stars/lichao-sun/Mora.svg?style=social&label=Star)
- **AesopAgent: Agent-driven Evolutionary System on Story-to-Video
  Production**, `arXiv, 2403.07952`, [arxiv](http://arxiv.org/abs/2403.07952v1), [pdf](http://arxiv.org/pdf/2403.07952v1.pdf), cication: [**-1**](None)

	 *Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao*
- **VideoElevator: Elevating Video Generation Quality with Versatile
  Text-to-Image Diffusion Models**, `arXiv, 2403.05438`, [arxiv](http://arxiv.org/abs/2403.05438v1), [pdf](http://arxiv.org/pdf/2403.05438v1.pdf), cication: [**-1**](None)

	 *Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo* · ([VideoElevator](https://github.com/YBYBZhang/VideoElevator) - YBYBZhang) ![Star](https://img.shields.io/github/stars/YBYBZhang/VideoElevator.svg?style=social&label=Star)
- **Pix2Gif: Motion-Guided Diffusion for GIF Generation**, `arXiv, 2403.04634`, [arxiv](http://arxiv.org/abs/2403.04634v1), [pdf](http://arxiv.org/pdf/2403.04634v1.pdf), cication: [**-1**](None)

	 *Hitesh Kandala, Jianfeng Gao, Jianwei Yang*
- [**Open-Sora**](https://github.com/hpcaitech/Open-Sora) - hpcaitech ![Star](https://img.shields.io/github/stars/hpcaitech/Open-Sora.svg?style=social&label=Star)

	 *Building your own video generation model like OpenAI's Sora*
- **Tuning-Free Noise Rectification for High Fidelity Image-to-Video
  Generation**, `arXiv, 2403.02827`, [arxiv](http://arxiv.org/abs/2403.02827v1), [pdf](http://arxiv.org/pdf/2403.02827v1.pdf), cication: [**-1**](None)

	 *Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng* · ([noise-rectification.github](https://noise-rectification.github.io))
- [**Open-Sora-Plan**](https://github.com/PKU-YuanGroup/Open-Sora-Plan) - PKU-YuanGroup ![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan.svg?style=social&label=Star)

	 *This project aim to reproducing Sora (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project.*
- **Sora Generates Videos with Stunning Geometrical Consistency**, `arXiv, 2402.17403`, [arxiv](http://arxiv.org/abs/2402.17403v1), [pdf](http://arxiv.org/pdf/2402.17403v1.pdf), cication: [**-1**](None)

	 *Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, Ming-Ming Cheng* · ([sora-geometrical-consistency.github](https://sora-geometrical-consistency.github.io/))
- **Sora: A Review on Background, Technology, Limitations, and Opportunities
  of Large Vision Models**, `arXiv, 2402.17177`, [arxiv](http://arxiv.org/abs/2402.17177v1), [pdf](http://arxiv.org/pdf/2402.17177v1.pdf), cication: [**-1**](None)

	 *Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao*
- **Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video
  Synthesis**, `arXiv, 2402.14797`, [arxiv](http://arxiv.org/abs/2402.14797v1), [pdf](http://arxiv.org/pdf/2402.14797v1.pdf), cication: [**-1**](None)

	 *Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren* · ([snap-research.github](https://snap-research.github.io/snapvideo/))
- [**AnimateLCM-SVD-xt**](https://huggingface.co/wangfuyun/AnimateLCM-SVD-xt) - wangfuyun 🤗
- [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)
- **Magic-Me: Identity-Specific Video Customized Diffusion**, `arXiv, 2402.09368`, [arxiv](http://arxiv.org/abs/2402.09368v1), [pdf](http://arxiv.org/pdf/2402.09368v1.pdf), cication: [**-1**](None)

	 *Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng* · ([Magic-Me](https://github.com/Zhen-Dong/Magic-Me) - Zhen-Dong) ![Star](https://img.shields.io/github/stars/Zhen-Dong/Magic-Me.svg?style=social&label=Star)
- **ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation**, `arXiv, 2402.04324`, [arxiv](http://arxiv.org/abs/2402.04324v1), [pdf](http://arxiv.org/pdf/2402.04324v1.pdf), cication: [**-1**](None)

	 *Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, Wenhu Chen*
- **Video-LaVIT: Unified Video-Language Pre-training with Decoupled
  Visual-Motional Tokenization**, `arXiv, 2402.03161`, [arxiv](http://arxiv.org/abs/2402.03161v2), [pdf](http://arxiv.org/pdf/2402.03161v2.pdf), cication: [**-1**](None)

	 *Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang* · ([video-lavit.github](https://video-lavit.github.i))
- **Direct-a-Video: Customized Video Generation with User-Directed Camera
  Movement and Object Motion**, `arXiv, 2402.03162`, [arxiv](http://arxiv.org/abs/2402.03162v1), [pdf](http://arxiv.org/pdf/2402.03162v1.pdf), cication: [**-1**](None)

	 *Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao* · ([direct-a-video.github](https://direct-a-video.github.io/))
- **Boximator: Generating Rich and Controllable Motions for Video Synthesis**, `arXiv, 2402.01566`, [arxiv](http://arxiv.org/abs/2402.01566v1), [pdf](http://arxiv.org/pdf/2402.01566v1.pdf), cication: [**-1**](None)

	 *Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, Hang Li*
- **InteractiveVideo: User-Centric Controllable Video Generation with
  Synergistic Multimodal Instructions**, `arXiv, 2402.03040`, [arxiv](http://arxiv.org/abs/2402.03040v1), [pdf](http://arxiv.org/pdf/2402.03040v1.pdf), cication: [**-1**](None)

	 *Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue* · ([InteractiveVideo](https://github.com/invictus717/InteractiveVideo) - invictus717) ![Star](https://img.shields.io/github/stars/invictus717/InteractiveVideo.svg?style=social&label=Star)
- **AnimateLCM: Accelerating the Animation of Personalized Diffusion Models
  and Adapters with Decoupled Consistency Learning**, `arXiv, 2402.00769`, [arxiv](http://arxiv.org/abs/2402.00769v1), [pdf](http://arxiv.org/pdf/2402.00769v1.pdf), cication: [**-1**](None)

	 *Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li* · ([AnimateLCM](https://github.com/G-U-N/AnimateLCM) - G-U-N) ![Star](https://img.shields.io/github/stars/G-U-N/AnimateLCM.svg?style=social&label=Star)
- **VideoCrafter2: Overcoming Data Limitations for High-Quality Video
  Diffusion Models**, `arXiv, 2401.09047`, [arxiv](http://arxiv.org/abs/2401.09047v1), [pdf](http://arxiv.org/pdf/2401.09047v1.pdf), cication: [**-1**](None)

	 *Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, Ying Shan* · ([VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) - AILab-CVC) ![Star](https://img.shields.io/github/stars/AILab-CVC/VideoCrafter.svg?style=social&label=Star)
- **Lumiere: A Space-Time Diffusion Model for Video Generation**, `arXiv, 2401.12945`, [arxiv](http://arxiv.org/abs/2401.12945v1), [pdf](http://arxiv.org/pdf/2401.12945v1.pdf), cication: [**-1**](None)

	 *Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli*
- **Inflation with Diffusion: Efficient Temporal Adaptation for
  Text-to-Video Super-Resolution**, `proceedings of the ieee/cvf winter conference on applications …, 2024`, [arxiv](http://arxiv.org/abs/2401.10404v1), [pdf](http://arxiv.org/pdf/2401.10404v1.pdf), cication: [**-1**](None)

	 *Xin Yuan, Jinoo Baek, Keyang Xu, Omer Tov, Hongliang Fei*
- **CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects**, `arXiv, 2401.09962`, [arxiv](http://arxiv.org/abs/2401.09962v1), [pdf](http://arxiv.org/pdf/2401.09962v1.pdf), cication: [**-1**](None)

	 *Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li*
- **WorldDreamer: Towards General World Models for Video Generation via
  Predicting Masked Tokens**, `arXiv, 2401.09985`, [arxiv](http://arxiv.org/abs/2401.09985v1), [pdf](http://arxiv.org/pdf/2401.09985v1.pdf), cication: [**-1**](None)

	 *Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu*
- **VideoCrafter2: Overcoming Data Limitations for High-Quality Video
  Diffusion Models**, `arXiv, 2401.09047`, [arxiv](http://arxiv.org/abs/2401.09047v1), [pdf](http://arxiv.org/pdf/2401.09047v1.pdf), cication: [**-1**](None)

	 *Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, Ying Shan* · ([VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) - AILab-CVC) ![Star](https://img.shields.io/github/stars/AILab-CVC/VideoCrafter.svg?style=social&label=Star) · ([ailab-cvc.github](https://ailab-cvc.github.io/videocrafter2)) · ([huggingface](https://huggingface.co/spaces/VideoCrafter/VideoCrafter2))
- **UniVG: Towards UNIfied-modal Video Generation**, `arXiv, 2401.09084`, [arxiv](http://arxiv.org/abs/2401.09084v1), [pdf](http://arxiv.org/pdf/2401.09084v1.pdf), cication: [**-1**](None)

	 *Ludan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao*
- **VideoCrafter2: Overcoming Data Limitations for High-Quality Video
  Diffusion Models**, `arXiv, 2401.09047`, [arxiv](http://arxiv.org/abs/2401.09047v1), [pdf](http://arxiv.org/pdf/2401.09047v1.pdf), cication: [**-1**](None)

	 *Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, Ying Shan* · ([VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) - AILab-CVC) ![Star](https://img.shields.io/github/stars/AILab-CVC/VideoCrafter.svg?style=social&label=Star) · ([ailab-cvc.github](https://ailab-cvc.github.io/videocrafter;))
- **Vlogger: Make Your Dream A Vlog**, `arXiv, 2401.09414`, [arxiv](http://arxiv.org/abs/2401.09414v1), [pdf](http://arxiv.org/pdf/2401.09414v1.pdf), cication: [**-1**](None)

	 *Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang* · ([Vlogger](https://github.com/zhuangshaobin/Vlogger) - zhuangshaobin) ![Star](https://img.shields.io/github/stars/zhuangshaobin/Vlogger.svg?style=social&label=Star)
- **UniVG: Towards UNIfied-modal Video Generation**, `arXiv, 2401.09084`, [arxiv](http://arxiv.org/abs/2401.09084v1), [pdf](http://arxiv.org/pdf/2401.09084v1.pdf), cication: [**-1**](None)

	 *Ludan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao* · ([univg-baidu.github](https://univg-baidu.github.io/))
- **Towards A Better Metric for Text-to-Video Generation**, `arXiv, 2401.07781`, [arxiv](http://arxiv.org/abs/2401.07781v1), [pdf](http://arxiv.org/pdf/2401.07781v1.pdf), cication: [**-1**](None)

	 *Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao Ge, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao*
- **Latte: Latent Diffusion Transformer for Video Generation**, `arXiv, 2401.03048`, [arxiv](http://arxiv.org/abs/2401.03048v1), [pdf](http://arxiv.org/pdf/2401.03048v1.pdf), cication: [**-1**](None)

	 *Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, Yu Qiao* · ([maxin-cn.github](https://maxin-cn.github.io/latte_project/)) · ([Latte](https://github.com/maxin-cn/Latte) - maxin-cn) ![Star](https://img.shields.io/github/stars/maxin-cn/Latte.svg?style=social&label=Star)
- **MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation**, `arXiv, 2401.04468`, [arxiv](http://arxiv.org/abs/2401.04468v1), [pdf](http://arxiv.org/pdf/2401.04468v1.pdf), cication: [**-1**](None)

	 *Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan* · ([magicvideov2.github](https://magicvideov2.github.io/))
- **AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated
  by AI**, `arXiv, 2401.01651`, [arxiv](http://arxiv.org/abs/2401.01651v3), [pdf](http://arxiv.org/pdf/2401.01651v3.pdf), cication: [**-1**](None)

	 *Fanda Fan, Chunjie Luo, Wanling Gao, Jianfeng Zhan*
- **Moonshot: Towards Controllable Video Generation and Editing with
  Multimodal Conditions**, `arXiv, 2401.01827`, [arxiv](http://arxiv.org/abs/2401.01827v1), [pdf](http://arxiv.org/pdf/2401.01827v1.pdf), cication: [**-1**](None)

	 *David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo* · ([LAVIS](https://github.com/salesforce/LAVIS) - salesforce) ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)
- **VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM**, `arXiv, 2401.01256`, [arxiv](http://arxiv.org/abs/2401.01256v1), [pdf](http://arxiv.org/pdf/2401.01256v1.pdf), cication: [**-1**](None)

	 *Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei*
- **TrailBlazer: Trajectory Control for Diffusion-Based Video Generation**, `arXiv, 2401.00896`, [arxiv](http://arxiv.org/abs/2401.00896v1), [pdf](http://arxiv.org/pdf/2401.00896v1.pdf), cication: [**-1**](None)

	 *Wan-Duo Kurt Ma, J. P. Lewis, W. Bastiaan Kleijn*
- **I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models**, `arXiv, 2312.16693`, [arxiv](http://arxiv.org/abs/2312.16693v2), [pdf](http://arxiv.org/pdf/2312.16693v2.pdf), cication: [**-1**](None)

	 *Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha*
- **A Recipe for Scaling up Text-to-Video Generation with Text-free Videos**, `arXiv, 2312.15770`, [arxiv](http://arxiv.org/abs/2312.15770v1), [pdf](http://arxiv.org/pdf/2312.15770v1.pdf), cication: [**-1**](None)

	 *Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, Nong Sang* · ([tf-t2v.github](https://tf-t2v.github.io/))
- **MotionCtrl: A Unified and Flexible Motion Controller for Video
  Generation**, `arXiv, 2312.03641`, [arxiv](http://arxiv.org/abs/2312.03641v1), [pdf](http://arxiv.org/pdf/2312.03641v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=7380005192683420267&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan* · ([MotionCtrl](https://github.com/TencentARC/MotionCtrl) - TencentARC) ![Star](https://img.shields.io/github/stars/TencentARC/MotionCtrl.svg?style=social&label=Star)
- **Generative AI Beyond LLMs: System Implications of Multi-Modal Generation**, `arXiv, 2312.14385`, [arxiv](http://arxiv.org/abs/2312.14385v1), [pdf](http://arxiv.org/pdf/2312.14385v1.pdf), cication: [**-1**](None)

	 *Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee, Zachary DeVito, Jeff Johnson, Gu-Yeon Wei, David Brooks*
- **InstructVideo: Instructing Video Diffusion Models with Human Feedback**, `arXiv, 2312.12490`, [arxiv](http://arxiv.org/abs/2312.12490v1), [pdf](http://arxiv.org/pdf/2312.12490v1.pdf), cication: [**-1**](None)

	 *Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni*
- **VideoPoet: A Large Language Model for Zero-Shot Video Generation**, `arXiv, 2312.14125`, [arxiv](http://arxiv.org/abs/2312.14125v1), [pdf](http://arxiv.org/pdf/2312.14125v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=5579274214599195084&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar* · ([blog.research](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html))
- **MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual
  Storytelling via Multi-Layered Semantic-Aware Denoising**, `arXiv, 2312.10899`, [arxiv](http://arxiv.org/abs/2312.10899v1), [pdf](http://arxiv.org/pdf/2312.10899v1.pdf), cication: [**-1**](None)

	 *Bingyuan Wang, Hengyu Meng, Zeyu Cai, Lanjiong Li, Yue Ma, Qifeng Chen, Zeyu Wang*
- **VideoLCM: Video Latent Consistency Model**, `arXiv, 2312.09109`, [arxiv](http://arxiv.org/abs/2312.09109v1), [pdf](http://arxiv.org/pdf/2312.09109v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=7371754451340416336&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, Nong Sang*
- **DreaMoving: A Human Video Generation Framework based on Diffusion Models**, `arXiv, 2312.05107`, [arxiv](http://arxiv.org/abs/2312.05107v2), [pdf](http://arxiv.org/pdf/2312.05107v2.pdf), cication: [**-1**](None)

	 *Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li* · ([dreamoving-project](https://github.com/dreamoving/dreamoving-project) - dreamoving) ![Star](https://img.shields.io/github/stars/dreamoving/dreamoving-project.svg?style=social&label=Star)
- **PEEKABOO: Interactive Video Generation via Masked-Diffusion**, `arXiv, 2312.07509`, [arxiv](http://arxiv.org/abs/2312.07509v1), [pdf](http://arxiv.org/pdf/2312.07509v1.pdf), cication: [**-1**](None)

	 *Yash Jain, Anshul Nasery, Vibhav Vineet, Harkirat Behl*
- **FreeInit: Bridging Initialization Gap in Video Diffusion Models**, `arXiv, 2312.07537`, [arxiv](http://arxiv.org/abs/2312.07537v1), [pdf](http://arxiv.org/pdf/2312.07537v1.pdf), cication: [**-1**](None)

	 *Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu* · ([FreeInit](https://github.com/TianxingWu/FreeInit) - TianxingWu) ![Star](https://img.shields.io/github/stars/TianxingWu/FreeInit.svg?style=social&label=Star) · ([tianxingwu.github](https://tianxingwu.github.io/pages/FreeInit/))
- **Photorealistic Video Generation with Diffusion Models**, `arXiv, 2312.06662`, [arxiv](http://arxiv.org/abs/2312.06662v1), [pdf](http://arxiv.org/pdf/2312.06662v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=10254571720384029658&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, José Lezama* · ([walt-video-diffusion.github](https://walt-video-diffusion.github.io/)) · ([walt-video-diffusion.github](https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf))
- **Customizing Motion in Text-to-Video Diffusion Models**, `arXiv, 2312.04966`, [arxiv](http://arxiv.org/abs/2312.04966v1), [pdf](http://arxiv.org/pdf/2312.04966v1.pdf), cication: [**-1**](None)

	 *Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, Bryan Russell*
- **DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention
  and Text Guidance**, `arXiv, 2312.03018`, [arxiv](http://arxiv.org/abs/2312.03018v3), [pdf](http://arxiv.org/pdf/2312.03018v3.pdf), cication: [**-1**](None)

	 *Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, Xiaodan Liang*
- **StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style
  Adapter**, `arXiv, 2312.00330`, [arxiv](http://arxiv.org/abs/2312.00330v1), [pdf](http://arxiv.org/pdf/2312.00330v1.pdf), cication: [**-1**](None)

	 *Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Xintao Wang, Yujiu Yang, Ying Shan*
- **BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis
  via Bridging Image and Video Diffusion Models**, `arXiv, 2312.02813`, [arxiv](http://arxiv.org/abs/2312.02813v1), [pdf](http://arxiv.org/pdf/2312.02813v1.pdf), cication: [**-1**](None)

	 *Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, Limin Wang*
- **DreamVideo: Composing Your Dream Videos with Customized Subject and
  Motion**, `arXiv, 2312.04433`, [arxiv](http://arxiv.org/abs/2312.04433v1), [pdf](http://arxiv.org/pdf/2312.04433v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=10405801990377929167&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan* · ([dreamvideo-t2v.github](https://dreamvideo-t2v.github.io))
- **Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation**, `arXiv, 2312.04483`, [arxiv](http://arxiv.org/abs/2312.04483v1), [pdf](http://arxiv.org/pdf/2312.04483v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=859069773317139129&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, Nong Sang* · ([higen-t2v.github](https://higen-t2v.github.io))
- **GenTron: Delving Deep into Diffusion Transformers for Image and Video
  Generation**, `arXiv, 2312.04557`, [arxiv](http://arxiv.org/abs/2312.04557v1), [pdf](http://arxiv.org/pdf/2312.04557v1.pdf), cication: [**-1**](None)

	 *Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, Juan-Manuel Perez-Rua*
- **MotionCtrl: A Unified and Flexible Motion Controller for Video
  Generation**, `arXiv, 2312.03641`, [arxiv](http://arxiv.org/abs/2312.03641v1), [pdf](http://arxiv.org/pdf/2312.03641v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=7380005192683420267&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan*
- **Fine-grained Controllable Video Generation via Object Appearance and
  Context**, `arXiv, 2312.02919`, [arxiv](http://arxiv.org/abs/2312.02919v1), [pdf](http://arxiv.org/pdf/2312.02919v1.pdf), cication: [**-1**](None)

	 *Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, Ming-Hsuan Yang*
- **VMC: Video Motion Customization using Temporal Attention Adaption for
  Text-to-Video Diffusion Models**, `arXiv, 2312.00845`, [arxiv](http://arxiv.org/abs/2312.00845v1), [pdf](http://arxiv.org/pdf/2312.00845v1.pdf), cication: [**-1**](None)

	 *Hyeonho Jeong, Geon Yeong Park, Jong Chul Ye* · ([Video-Motion-Customization](https://github.com/HyeonHo99/Video-Motion-Customization) - HyeonHo99) ![Star](https://img.shields.io/github/stars/HyeonHo99/Video-Motion-Customization.svg?style=social&label=Star) · ([video-motion-customization.github](https://video-motion-customization.github.io/))
- **VideoBooth: Diffusion-based Video Generation with Image Prompts**, `arXiv, 2312.00777`, [arxiv](http://arxiv.org/abs/2312.00777v1), [pdf](http://arxiv.org/pdf/2312.00777v1.pdf), cication: [**-1**](None)

	 *Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, Ziwei Liu*
- **MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation**, `arXiv, 2311.18829`, [arxiv](http://arxiv.org/abs/2311.18829v2), [pdf](http://arxiv.org/pdf/2311.18829v2.pdf), cication: [**-1**](None)

	 *Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu Zhang, Qi Dai Zhiyuan Zhao, Chunyu Wang, Kai Qiu*
- **I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion
  Models**, `arXiv, 2311.04145`, [arxiv](http://arxiv.org/abs/2311.04145v1), [pdf](http://arxiv.org/pdf/2311.04145v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=5568396991815087432&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, Jingren Zhou* · ([i2vgen-xl.github](https://i2vgen-xl.github.io/)) · ([huggingface](https://huggingface.co/spaces/damo-vilab/I2VGen-XL)) · ([i2vgen-xl](https://github.com/damo-vilab/i2vgen-xl) - damo-vilab) ![Star](https://img.shields.io/github/stars/damo-vilab/i2vgen-xl.svg?style=social&label=Star)
- **FusionFrames: Efficient Architectural Aspects for Text-to-Video
  Generation Pipeline**, `arXiv, 2311.13073`, [arxiv](http://arxiv.org/abs/2311.13073v2), [pdf](http://arxiv.org/pdf/2311.13073v2.pdf), cication: [**-1**](None)

	 *Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov* · ([ai-forever.github](https://ai-forever.github.io/kandinsky-video/)) · ([kandinskyvideo](https://github.com/ai-forever/kandinskyvideo) - ai-forever) ![Star](https://img.shields.io/github/stars/ai-forever/kandinskyvideo.svg?style=social&label=Star)
- [Stable Video Diffusion ](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)

	 · ([huggingface](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid)) · ([generative-models](https://github.com/Stability-AI/generative-models) - Stability-AI) ![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/multimodalart/stable-video-diffusion))
- **MoVideo: Motion-Aware Video Generation with Diffusion Models**, `arXiv, 2311.11325`, [arxiv](http://arxiv.org/abs/2311.11325v1), [pdf](http://arxiv.org/pdf/2311.11325v1.pdf), cication: [**-1**](None)

	 *Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, Rakesh Ranjan* · ([jingyunliang.github](https://jingyunliang.github.io/MoVideo/))
- **Emu Video: Factorizing Text-to-Video Generation by Explicit Image
  Conditioning**, `arXiv, 2311.10709`, [arxiv](http://arxiv.org/abs/2311.10709v1), [pdf](http://arxiv.org/pdf/2311.10709v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=3233153568472686810&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra* · ([emu-video.metademolab](https://emu-video.metademolab.com/)) · ([emu-video.metademolab](https://emu-video.metademolab.com/assets/emu_video.pdf))
- **FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain
  Text-to-Video Generation**, `arXiv, 2311.01813`, [arxiv](http://arxiv.org/abs/2311.01813v3), [pdf](http://arxiv.org/pdf/2311.01813v3.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=16797871499191104183&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, Lu Hou* · ([FETV](https://github.com/llyx97/FETV) - llyx97) ![Star](https://img.shields.io/github/stars/llyx97/FETV.svg?style=social&label=Star)
- **FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling**, `arXiv, 2310.15169`, [arxiv](http://arxiv.org/abs/2310.15169v3), [pdf](http://arxiv.org/pdf/2310.15169v3.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=14113267982552847374&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu* · ([qbitai](https://www.qbitai.com/2024/01/116406.html))
- **MotionDirector: Motion Customization of Text-to-Video Diffusion Models**, `arXiv, 2310.08465`, [arxiv](http://arxiv.org/abs/2310.08465v1), [pdf](http://arxiv.org/pdf/2310.08465v1.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=16973388104782535447&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, Mike Zheng Shou* · ([MotionDirector](https://github.com/showlab/MotionDirector) - showlab) ![Star](https://img.shields.io/github/stars/showlab/MotionDirector.svg?style=social&label=Star)
- **LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion
  Models**, `arXiv, 2309.15103`, [arxiv](http://arxiv.org/abs/2309.15103v2), [pdf](http://arxiv.org/pdf/2309.15103v2.pdf), cication: [**23**](https://scholar.google.com/scholar?cites=2163005604050927408&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang* · ([LaVie](https://github.com/Vchitect/LaVie) - Vchitect) ![Star](https://img.shields.io/github/stars/Vchitect/LaVie.svg?style=social&label=Star)

## Animation
- **TCAN: Animating Human Images with Temporally Consistent Pose Guidance
  using Diffusion Models**, `arXiv, 2407.09012`, [arxiv](http://arxiv.org/abs/2407.09012v1), [pdf](http://arxiv.org/pdf/2407.09012v1.pdf), cication: [**-1**](None)

	 *Jeongho Kim, Min-Jung Kim, Junsoo Lee, Jaegul Choo* · ([eccv2024tcan.github](https://eccv2024tcan.github.io/))
- **Still-Moving: Customized Video Generation without Customized Video Data**, `arXiv, 2407.08674`, [arxiv](http://arxiv.org/abs/2407.08674v1), [pdf](http://arxiv.org/pdf/2407.08674v1.pdf), cication: [**-1**](None)

	 *Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, Inbar Mosseri* · ([still-moving.github](https://still-moving.github.io/))
- **EasyAnimate: A High-Performance Long Video Generation Method based on
  Transformer Architecture**, `arXiv, 2405.18991`, [arxiv](http://arxiv.org/abs/2405.18991v2), [pdf](http://arxiv.org/pdf/2405.18991v2.pdf), cication: [**-1**](None)

	 *Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, Jun Huang* · ([EasyAnimate](https://github.com/aigc-apps/EasyAnimate) - aigc-apps) ![Star](https://img.shields.io/github/stars/aigc-apps/EasyAnimate.svg?style=social&label=Star)
- **MimicMotion: High-Quality Human Motion Video Generation with
  Confidence-aware Pose Guidance**, `arXiv, 2406.19680`, [arxiv](http://arxiv.org/abs/2406.19680v1), [pdf](http://arxiv.org/pdf/2406.19680v1.pdf), cication: [**-1**](None)

	 *Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou*

	 · ([MimicMotion](https://github.com/tencent/MimicMotion) - tencent) ![Star](https://img.shields.io/github/stars/tencent/MimicMotion.svg?style=social&label=Star) · ([tencent.github](https://tencent.github.io/MimicMotion/))
- **MOFA-Video: Controllable Image Animation via Generative Motion Field
  Adaptions in Frozen Image-to-Video Diffusion Model**, `arXiv, 2405.20222`, [arxiv](http://arxiv.org/abs/2405.20222v2), [pdf](http://arxiv.org/pdf/2405.20222v2.pdf), cication: [**-1**](None)

	 *Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, Yinqiang Zheng* · ([mofa-video](https://github.com/myniuuu/mofa-video) - myniuuu) ![Star](https://img.shields.io/github/stars/myniuuu/mofa-video.svg?style=social&label=Star)
- **Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation
  for Stable Pose Control**, `arXiv, 2406.03035`, [arxiv](http://arxiv.org/abs/2406.03035v2), [pdf](http://arxiv.org/pdf/2406.03035v2.pdf), cication: [**-1**](None)

	 *Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum* · ([huggingface](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT?continueFlag=7b93f8f875055859b0712c994d253b9f)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s/z8wKg0jOJ7tgjPMUV3N0-g))
- **UniAnimate: Taming Unified Video Diffusion Models for Consistent Human
  Image Animation**, `arXiv, 2406.01188`, [arxiv](http://arxiv.org/abs/2406.01188v1), [pdf](http://arxiv.org/pdf/2406.01188v1.pdf), cication: [**-1**](None)

	 *Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, Nong Sang* · ([UniAnimate](https://github.com/ali-vilab/UniAnimate) - ali-vilab) ![Star](https://img.shields.io/github/stars/ali-vilab/UniAnimate.svg?style=social&label=Star) · ([unianimate.github](https://unianimate.github.io/))
- [**ToonCrafter**](https://github.com/ToonCrafter/ToonCrafter) - ToonCrafter ![Star](https://img.shields.io/github/stars/ToonCrafter/ToonCrafter.svg?style=social&label=Star)

	 *a research paper for generative cartoon interpolation*
- **SignLLM: Sign Languages Production Large Language Models**, `arXiv, 2405.10718`, [arxiv](http://arxiv.org/abs/2405.10718v1), [pdf](http://arxiv.org/pdf/2405.10718v1.pdf), cication: [**-1**](None)

	 *Sen Fang, Lei Wang, Ce Zheng, Yapeng Tian, Chen Chen* · ([signllm.github](https://signllm.github.io/))
- [**MusePose**](https://github.com/TMElyralab/MusePose) - TMElyralab ![Star](https://img.shields.io/github/stars/TMElyralab/MusePose.svg?style=social&label=Star)

	 *MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation*
- **MOFA-Video: Controllable Image Animation via Generative Motion Field
  Adaptions in Frozen Image-to-Video Diffusion Model**, `arXiv, 2405.20222`, [arxiv](http://arxiv.org/abs/2405.20222v1), [pdf](http://arxiv.org/pdf/2405.20222v1.pdf), cication: [**-1**](None)

	 *Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, Yinqiang Zheng*
- **EasyAnimate: A High-Performance Long Video Generation Method based on
  Transformer Architecture**, `arXiv, 2405.18991`, [arxiv](http://arxiv.org/abs/2405.18991v1), [pdf](http://arxiv.org/pdf/2405.18991v1.pdf), cication: [**-1**](None)

	 *Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, Jun Huang*
- **VividPose: Advancing Stable Video Diffusion for Realistic Human Image
  Animation**, `arXiv, 2405.18156`, [arxiv](http://arxiv.org/abs/2405.18156v1), [pdf](http://arxiv.org/pdf/2405.18156v1.pdf), cication: [**-1**](None)

	 *Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, Yanwei Fu*
- **CamViG: Camera Aware Image-to-Video Generation with Multimodal
  Transformers**, `arXiv, 2405.13195`, [arxiv](http://arxiv.org/abs/2405.13195v1), [pdf](http://arxiv.org/pdf/2405.13195v1.pdf), cication: [**-1**](None)

	 *Andrew Marmon, Grant Schindler, José Lezama, Dan Kondratyuk, Bryan Seybold, Irfan Essa*
- **Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video
  Motion Editing**, `arXiv, 2405.04496`, [arxiv](http://arxiv.org/abs/2405.04496v1), [pdf](http://arxiv.org/pdf/2405.04496v1.pdf), cication: [**-1**](None)

	 *Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, Yuwei Guo*
- **ID-Animator: Zero-Shot Identity-Preserving Human Video Generation**, `arXiv, 2404.15275`, [arxiv](http://arxiv.org/abs/2404.15275v1), [pdf](http://arxiv.org/pdf/2404.15275v1.pdf), cication: [**-1**](None)

	 *Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, Jie Zhang* · ([id-animator.github](https://id-animator.github.io/)) · ([ID-Animator](https://github.com/ID-Animator/ID-Animator) - ID-Animator) ![Star](https://img.shields.io/github/stars/ID-Animator/ID-Animator.svg?style=social&label=Star)
- **Dynamic Typography: Bringing Text to Life via Video Diffusion Prior**, `arXiv, 2404.11614`, [arxiv](http://arxiv.org/abs/2404.11614v2), [pdf](http://arxiv.org/pdf/2404.11614v2.pdf), cication: [**-1**](None)

	 *Zichen Liu, Yihao Meng, Hao Ouyang, Yue Yu, Bolin Zhao, Daniel Cohen-Or, Huamin Qu* · ([animate-your-word.github](https://animate-your-word.github.io/demo))
- **AniClipart: Clipart Animation with Text-to-Video Priors**, `arXiv, 2404.12347`, [arxiv](http://arxiv.org/abs/2404.12347v1), [pdf](http://arxiv.org/pdf/2404.12347v1.pdf), cication: [**-1**](None)

	 *Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao*

	 · ([aniclipart.github](https://aniclipart.github.io/))
- **AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via
  Subject Alignment**, `arXiv, 2404.04946`, [arxiv](http://arxiv.org/abs/2404.04946v1), [pdf](http://arxiv.org/pdf/2404.04946v1.pdf), cication: [**-1**](None)

	 *Yuanfeng Xu, Yuhao Chen, Zhongzhan Huang, Zijian He, Guangrun Wang, Philip Torr, Liang Lin* · ([AnimateZoo](https://github.com/JustinXu0/AnimateZoo) - JustinXu0) ![Star](https://img.shields.io/github/stars/JustinXu0/AnimateZoo.svg?style=social&label=Star) · ([justinxu0.github](https://justinxu0.github.io/AnimateZoo/))
- **TRIP: Temporal Residual Learning with Image Noise Prior for
  Image-to-Video Diffusion Models**, `arXiv, 2403.17005`, [arxiv](http://arxiv.org/abs/2403.17005v1), [pdf](http://arxiv.org/pdf/2403.17005v1.pdf), cication: [**-1**](None)

	 *Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei*

	 · ([trip-i2v.github](https://trip-i2v.github.io/TRIP/))
- **Champ: Controllable and Consistent Human Image Animation with 3D
  Parametric Guidance**, `arXiv, 2403.14781`, [arxiv](http://arxiv.org/abs/2403.14781v1), [pdf](http://arxiv.org/pdf/2403.14781v1.pdf), cication: [**-1**](None)

	 *Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, Siyu Zhu* · ([fudan-generative-vision.github](https://fudan-generative-vision.github.io/champ)) · ([champ](https://github.com/fudan-generative-vision/champ) - fudan-generative-vision) ![Star](https://img.shields.io/github/stars/fudan-generative-vision/champ.svg?style=social&label=Star)
- **Explorative Inbetweening of Time and Space**, `arXiv, 2403.14611`, [arxiv](http://arxiv.org/abs/2403.14611v1), [pdf](http://arxiv.org/pdf/2403.14611v1.pdf), cication: [**-1**](None)

	 *Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J. Black, Xuaner Zhang* · ([time-reversal.github](https://time-reversal.github.io))
- **StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained
  StyleGAN**, `arXiv, 2403.14186`, [arxiv](http://arxiv.org/abs/2403.14186v1), [pdf](http://arxiv.org/pdf/2403.14186v1.pdf), cication: [**-1**](None)

	 *Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh*
- **AnimateDiff-Lightning: Cross-Model Diffusion Distillation**, `arXiv, 2403.12706`, [arxiv](http://arxiv.org/abs/2403.12706v1), [pdf](http://arxiv.org/pdf/2403.12706v1.pdf), cication: [**-1**](None)

	 *Shanchuan Lin, Xiao Yang*

	 · ([huggingface](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning))
- **Animate Your Motion: Turning Still Images into Dynamic Videos**, `arXiv, 2403.10179`, [arxiv](http://arxiv.org/abs/2403.10179v1), [pdf](http://arxiv.org/pdf/2403.10179v1.pdf), cication: [**-1**](None)

	 *Mingxiao Li, Bo Wan, Marie-Francine Moens, Tinne Tuytelaars*
- **WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text
  and Image Inputs**, `arXiv, 2403.07944`, [arxiv](http://arxiv.org/abs/2403.07944v1), [pdf](http://arxiv.org/pdf/2403.07944v1.pdf), cication: [**-1**](None)

	 *Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang, Cindy Yang, Yuexian Zou*
- **Follow-Your-Click: Open-domain Regional Image Animation via Short
  Prompts**, `arXiv, 2403.08268`, [arxiv](http://arxiv.org/abs/2403.08268v1), [pdf](http://arxiv.org/pdf/2403.08268v1.pdf), cication: [**-1**](None)

	 *Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu* · ([follow-your-click.github](https://follow-your-click.github.io/))
- **Audio-Synchronized Visual Animation**, `arXiv, 2403.05659`, [arxiv](http://arxiv.org/abs/2403.05659v1), [pdf](http://arxiv.org/pdf/2403.05659v1.pdf), cication: [**-1**](None)

	 *Lin Zhang, Shentong Mo, Yijing Zhang, Pedro Morgado*

	 · ([lzhangbj.github](https://lzhangbj.github.io/projects/asva/asva.html))
- **DragAnything: Motion Control for Anything using Entity Representation**, `arXiv, 2403.07420`, [arxiv](http://arxiv.org/abs/2403.07420v2), [pdf](http://arxiv.org/pdf/2403.07420v2.pdf), cication: [**-1**](None)

	 *Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, Di Zhang*
- **AtomoVideo: High Fidelity Image-to-Video Generation**, `arXiv, 2403.01800`, [arxiv](http://arxiv.org/abs/2403.01800v2), [pdf](http://arxiv.org/pdf/2403.01800v2.pdf), cication: [**-1**](None)

	 *Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge, Bo Zheng* · ([atomo-video.github](https://atomo-video.github.io/))
- **Animated Stickers: Bringing Stickers to Life with Video Diffusion**, `arXiv, 2402.06088`, [arxiv](http://arxiv.org/abs/2402.06088v1), [pdf](http://arxiv.org/pdf/2402.06088v1.pdf), cication: [**-1**](None)

	 *David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard*
- **Motion-I2V: Consistent and Controllable Image-to-Video Generation with
  Explicit Motion Modeling**, `arXiv, 2401.15977`, [arxiv](http://arxiv.org/abs/2401.15977v2), [pdf](http://arxiv.org/pdf/2401.15977v2.pdf), cication: [**-1**](None)

	 *Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin*
- **Do You Guys Want to Dance: Zero-Shot Compositional Human Dance
  Generation with Multiple Persons**, `arXiv, 2401.13363`, [arxiv](http://arxiv.org/abs/2401.13363v1), [pdf](http://arxiv.org/pdf/2401.13363v1.pdf), cication: [**-1**](None)

	 *Zhe Xu, Kun Wei, Xu Yang, Cheng Deng*
- **Synthesizing Moving People with 3D Control**, `arXiv, 2401.10889`, [arxiv](http://arxiv.org/abs/2401.10889v1), [pdf](http://arxiv.org/pdf/2401.10889v1.pdf), cication: [**-1**](None)

	 *Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik* · ([boyiliee.github](https://boyiliee.github.io/3DHM.github.io/))
- **Continuous Piecewise-Affine Based Motion Model for Image Animation**, `arXiv, 2401.09146`, [arxiv](http://arxiv.org/abs/2401.09146v1), [pdf](http://arxiv.org/pdf/2401.09146v1.pdf), cication: [**-1**](None)

	 *Hexiang Wang, Fengqi Liu, Qianyu Zhou, Ran Yi, Xin Tan, Lizhuang Ma*
- [Motionshop-Replace the characters in video with 3D avatars](https://aigc3d.github.io/motionshop/)
- [**Moore-AnimateAnyone**](https://github.com/MooreThreads/Moore-AnimateAnyone?tab=readme-ov-file#-gradio-demo) - MooreThreads ![Star](https://img.shields.io/github/stars/MooreThreads/Moore-AnimateAnyone.svg?style=social&label=Star)
- [**LongAnimateDiff**](https://github.com/Lightricks/LongAnimateDiff) - Lightricks ![Star](https://img.shields.io/github/stars/Lightricks/LongAnimateDiff.svg?style=social&label=Star)

	 · ([huggingface](https://huggingface.co/spaces/Lightricks/LongAnimateDiff))
- **WonderJourney: Going from Anywhere to Everywhere**, `arXiv, 2312.03884`, [arxiv](http://arxiv.org/abs/2312.03884v1), [pdf](http://arxiv.org/pdf/2312.03884v1.pdf), cication: [**-1**](None)

	 *Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T. Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu* · ([WonderJourney](https://github.com/KovenYu/WonderJourney) - KovenYu) ![Star](https://img.shields.io/github/stars/KovenYu/WonderJourney.svg?style=social&label=Star) · ([kovenyu](https://kovenyu.com/wonderjourney/)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652418572&idx=4&sn=8e325065ee30dd09cefef21ced19e3b7))
- **Animate124: Animating One Image to 4D Dynamic Scene**, `arXiv, 2311.14603`, [arxiv](http://arxiv.org/abs/2311.14603v1), [pdf](http://arxiv.org/pdf/2311.14603v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=47125270228146871&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, Gim Hee Lee* · ([Animate124](https://github.com/HeliosZhao/Animate124) - HeliosZhao) ![Star](https://img.shields.io/github/stars/HeliosZhao/Animate124.svg?style=social&label=Star) · ([animate124.github](https://animate124.github.io/))
- **DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors**, `arXiv, 2310.12190`, [arxiv](http://arxiv.org/abs/2310.12190v2), [pdf](http://arxiv.org/pdf/2310.12190v2.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=6804708590492344727&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, Ying Shan* · ([doubiiu.github](https://doubiiu.github.io/projects/DynamiCrafter/)) · ([DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter) - Doubiiu) ![Star](https://img.shields.io/github/stars/Doubiiu/DynamiCrafter.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/Doubiiu/DynamiCrafter))

	 · ([huggingface](https://huggingface.co/Doubiiu/DynamiCrafter_1024))

	 · ([huggingface](https://huggingface.co/spaces/Doubiiu/DynamiCrafter_interp_loop))
- **AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**, `arXiv, 2312.03793`, [arxiv](http://arxiv.org/abs/2312.03793v1), [pdf](http://arxiv.org/pdf/2312.03793v1.pdf), cication: [**-1**](None)

	 *Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, Jian Zhang* · ([vvictoryuki.github](https://vvictoryuki.github.io/animatezero.github.io/))
- **LivePhoto: Real Image Animation with Text-guided Motion Control**, `arXiv, 2312.02928`, [arxiv](http://arxiv.org/abs/2312.02928v1), [pdf](http://arxiv.org/pdf/2312.02928v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=8753439023160233505&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao* · ([LivePhoto](https://github.com/XavierCHEN34/LivePhoto) - XavierCHEN34) ![Star](https://img.shields.io/github/stars/XavierCHEN34/LivePhoto.svg?style=social&label=Star) · ([xavierchen34.github](https://xavierchen34.github.io/LivePhoto-Page/))
- **Generative Rendering: Controllable 4D-Guided Video Generation with 2D
  Diffusion Models**, `arXiv, 2312.01409`, [arxiv](http://arxiv.org/abs/2312.01409v1), [pdf](http://arxiv.org/pdf/2312.01409v1.pdf), cication: [**-1**](None)

	 *Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Paul Huang, Tuanfeng Yang Wang, Gordon Wetzstein*
- **PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models**, `arXiv, 2312.13964`, [arxiv](http://arxiv.org/abs/2312.13964v3), [pdf](http://arxiv.org/pdf/2312.13964v3.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=17538941768586839512&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen* · ([PIA](https://github.com/open-mmlab/PIA) - open-mmlab) ![Star](https://img.shields.io/github/stars/open-mmlab/PIA.svg?style=social&label=Star)
- **MagicAnimate: Temporally Consistent Human Image Animation using
  Diffusion Model**, `arXiv, 2311.16498`, [arxiv](http://arxiv.org/abs/2311.16498v1), [pdf](http://arxiv.org/pdf/2311.16498v1.pdf), cication: [**-1**](None)

	 *Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, Mike Zheng Shou* · ([magic-animate](https://github.com/magic-research/magic-animate) - magic-research) ![Star](https://img.shields.io/github/stars/magic-research/magic-animate.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/zcxu-eric/magicanimate)) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-12-05-4)) · ([magic-animate-for-windows](https://github.com/sdbds/magic-animate-for-windows) - sdbds) ![Star](https://img.shields.io/github/stars/sdbds/magic-animate-for-windows.svg?style=social&label=Star)
- **Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for
  Character Animation**, `arXiv, 2311.17117`, [arxiv](http://arxiv.org/abs/2311.17117v2), [pdf](http://arxiv.org/pdf/2311.17117v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=9648438096292345618&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, Liefeng Bo* · ([AnimateAnyone](https://github.com/HumanAIGC/AnimateAnyone) - HumanAIGC) ![Star](https://img.shields.io/github/stars/HumanAIGC/AnimateAnyone.svg?style=social&label=Star) · ([AnimateAnyone-unofficial](https://github.com/guoqincode/AnimateAnyone-unofficial) - guoqincode) ![Star](https://img.shields.io/github/stars/guoqincode/AnimateAnyone-unofficial.svg?style=social&label=Star)
- **SEINE: Short-to-Long Video Diffusion Model for Generative Transition and
  Prediction**, `arXiv, 2310.20700`, [arxiv](http://arxiv.org/abs/2310.20700v2), [pdf](http://arxiv.org/pdf/2310.20700v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=6072100883560598995&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, Ziwei Liu* · ([SEINE](https://github.com/Vchitect/SEINE) - Vchitect) ![Star](https://img.shields.io/github/stars/Vchitect/SEINE.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/Vchitect/SEINE))
- **MagicDance: Realistic Human Dance Video Generation with Motions & Facial
  Expressions Transfer**, `arXiv, 2311.12052`, [arxiv](http://arxiv.org/abs/2311.12052v1), [pdf](http://arxiv.org/pdf/2311.12052v1.pdf), cication: [**-1**](None)

	 *Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, Mohammad Soleymani*
- **Make Pixels Dance: High-Dynamic Video Generation**, `arXiv, 2311.10982`, [arxiv](http://arxiv.org/abs/2311.10982v1), [pdf](http://arxiv.org/pdf/2311.10982v1.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=12237938157359928502&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, Hang Li* · ([makepixelsdance.github](https://makepixelsdance.github.io/))
- **DragNUWA: Fine-grained Control in Video Generation by Integrating Text,
  Image, and Trajectory**, `arXiv, 2308.08089`, [arxiv](http://arxiv.org/abs/2308.08089v1), [pdf](http://arxiv.org/pdf/2308.08089v1.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=5441474536222370175&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, Nan Duan* · ([DragNUWA](https://github.com/ProjectNUWA/DragNUWA) - ProjectNUWA) ![Star](https://img.shields.io/github/stars/ProjectNUWA/DragNUWA.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/yinsming/DragNUWA))
- **AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models
  without Specific Tuning**, `arXiv, 2307.04725`, [arxiv](http://arxiv.org/abs/2307.04725v1), [pdf](http://arxiv.org/pdf/2307.04725v1.pdf), cication: [**60**](https://scholar.google.com/scholar?cites=17668344440294432493&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai* · ([AnimateDiff](https://github.com/guoyww/AnimateDiff) - guoyww) ![Star](https://img.shields.io/github/stars/guoyww/AnimateDiff.svg?style=social&label=Star)
- **Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free
  Videos**, `arXiv, 2304.01186`, [arxiv](http://arxiv.org/abs/2304.01186v2), [pdf](http://arxiv.org/pdf/2304.01186v2.pdf), cication: [**31**](https://scholar.google.com/scholar?cites=13651603424932960809&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen* · ([FollowYourPose](https://github.com/mayuelala/FollowYourPose) - mayuelala) ![Star](https://img.shields.io/github/stars/mayuelala/FollowYourPose.svg?style=social&label=Star)
## Video Editting
- **Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models**, `arXiv, 2401.16224`, [arxiv](http://arxiv.org/abs/2401.16224v1), [pdf](http://arxiv.org/pdf/2401.16224v1.pdf), cication: [**-1**](None)

	 *Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang* · ([DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/Diffutoon) - modelscope) ![Star](https://img.shields.io/github/stars/modelscope/DiffSynth-Studio.svg?style=social&label=Star)
- **VIA: A Spatiotemporal Video Adaptation Framework for Global and Local
  Video Editing**, `arXiv, 2406.12831`, [arxiv](http://arxiv.org/abs/2406.12831v1), [pdf](http://arxiv.org/pdf/2406.12831v1.pdf), cication: [**-1**](None)

	 *Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, Xin Eric Wang*
- **VMC: Video Motion Customization using Temporal Attention Adaption for
  Text-to-Video Diffusion Models**, `arXiv, 2312.00845`, [arxiv](http://arxiv.org/abs/2312.00845v1), [pdf](http://arxiv.org/pdf/2312.00845v1.pdf), cication: [**-1**](None)

	 *Hyeonho Jeong, Geon Yeong Park, Jong Chul Ye* · ([video-motion-customization.github](https://video-motion-customization.github.io/)) · ([Video-Motion-Customization](https://github.com/HyeonHo99/Video-Motion-Customization) - HyeonHo99) ![Star](https://img.shields.io/github/stars/HyeonHo99/Video-Motion-Customization.svg?style=social&label=Star)
- **NaRCan: Natural Refined Canonical Image with Integration of Diffusion
  Prior for Video Editing**, `arXiv, 2406.06523`, [arxiv](http://arxiv.org/abs/2406.06523v1), [pdf](http://arxiv.org/pdf/2406.06523v1.pdf), cication: [**-1**](None)

	 *Ting-Hsuan Chen, Jiewen Chan, Hau-Shiang Shiu, Shih-Han Yen, Chang-Han Yeh, Yu-Lun Liu* · ([koi953215.github](https://koi953215.github.io/NaRCan_page/))
- **ReVideo: Remake a Video with Motion and Content Control**, `arXiv, 2405.13865`, [arxiv](http://arxiv.org/abs/2405.13865v1), [pdf](http://arxiv.org/pdf/2405.13865v1.pdf), cication: [**-1**](None)

	 *Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, Jian Zhang* · ([mc-e.github](https://mc-e.github.io/project/ReVideo/))
- **AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks**, `arXiv, 2403.14468`, [arxiv](http://arxiv.org/abs/2403.14468v1), [pdf](http://arxiv.org/pdf/2403.14468v1.pdf), cication: [**-1**](None)

	 *Max Ku, Cong Wei, Weiming Ren, Huan Yang, Wenhu Chen*

	 · ([tiger-ai-lab.github](https://tiger-ai-lab.github.io/AnyV2V/)) · ([AnyV2V](https://github.com/TIGER-AI-Lab/AnyV2V) - TIGER-AI-Lab) ![Star](https://img.shields.io/github/stars/TIGER-AI-Lab/AnyV2V.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/spaces/TIGER-Lab/AnyV2V))
- **Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific
  Adaptation**, `arXiv, 2403.13745`, [arxiv](http://arxiv.org/abs/2403.13745v1), [pdf](http://arxiv.org/pdf/2403.13745v1.pdf), cication: [**-1**](None)

	 *Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li*
- **FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation**, `arXiv, 2403.12962`, [arxiv](http://arxiv.org/abs/2403.12962v1), [pdf](http://arxiv.org/pdf/2403.12962v1.pdf), cication: [**-1**](None)

	 *Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy*

	 · ([fresco](https://github.com/williamyang1991/fresco) - williamyang1991) ![Star](https://img.shields.io/github/stars/williamyang1991/fresco.svg?style=social&label=Star)
- **Video Editing via Factorized Diffusion Distillation**, `arXiv, 2403.09334`, [arxiv](http://arxiv.org/abs/2403.09334v1), [pdf](http://arxiv.org/pdf/2403.09334v1.pdf), cication: [**-1**](None)

	 *Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, Yaniv Taigman*
- **FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video
  Editing**, `arXiv, 2403.06269`, [arxiv](http://arxiv.org/abs/2403.06269v1), [pdf](http://arxiv.org/pdf/2403.06269v1.pdf), cication: [**-1**](None)

	 *Youyuan Zhang, Xuan Ju, James J. Clark*
- **Anything in Any Scene: Photorealistic Video Object Insertion**, `arXiv, 2401.17509`, [arxiv](http://arxiv.org/abs/2401.17509v1), [pdf](http://arxiv.org/pdf/2401.17509v1.pdf), cication: [**-1**](None)

	 *Chen Bai, Zeman Shao, Guoxiang Zhang, Di Liang, Jie Yang, Zhuorui Zhang, Yujian Guo, Chengzhang Zhong, Yiqiao Qiu, Zhendong Wang* · ([anythinginanyscene.github](https://anythinginanyscene.github.io))
- **ActAnywhere: Subject-Aware Video Background Generation**, `arXiv, 2401.10822`, [arxiv](http://arxiv.org/abs/2401.10822v1), [pdf](http://arxiv.org/pdf/2401.10822v1.pdf), cication: [**-1**](None)

	 *Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang* · ([actanywhere.github](https://actanywhere.github.io/))
- **Object-Centric Diffusion for Efficient Video Editing**, `arXiv, 2401.05735`, [arxiv](http://arxiv.org/abs/2401.05735v1), [pdf](http://arxiv.org/pdf/2401.05735v1.pdf), cication: [**-1**](None)

	 *Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki M. Asano, Amirhossein Habibian*
- **FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video
  Synthesis**, `arXiv, 2312.17681`, [arxiv](http://arxiv.org/abs/2312.17681v1), [pdf](http://arxiv.org/pdf/2312.17681v1.pdf), cication: [**-1**](None)

	 *Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda* · ([jeff-liangf.github](https://jeff-liangf.github.io/projects/flowvid/))
- **Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis**, `arXiv, 2312.13834`, [arxiv](http://arxiv.org/abs/2312.13834v1), [pdf](http://arxiv.org/pdf/2312.13834v1.pdf), cication: [**-1**](None)

	 *Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, Peter Vajda*
- **MaskINT: Video Editing via Interpolative Non-autoregressive Masked
  Transformers**, `arXiv, 2312.12468`, [arxiv](http://arxiv.org/abs/2312.12468v1), [pdf](http://arxiv.org/pdf/2312.12468v1.pdf), cication: [**-1**](None)

	 *Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu, Wenliang Zhao, Lior Shapira, Xiaohui Xie*
- **VidToMe: Video Token Merging for Zero-Shot Video Editing**, `arXiv, 2312.10656`, [arxiv](http://arxiv.org/abs/2312.10656v2), [pdf](http://arxiv.org/pdf/2312.10656v2.pdf), cication: [**-1**](None)

	 *Xirui Li, Chao Ma, Xiaokang Yang, Ming-Hsuan Yang*
- **RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing
  with Diffusion Models**, `arXiv, 2312.04524`, [arxiv](http://arxiv.org/abs/2312.04524v1), [pdf](http://arxiv.org/pdf/2312.04524v1.pdf), cication: [**-1**](None)

	 *Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, Pinar Yanardag* · ([RAVE](https://github.com/rehg-lab/RAVE) - rehg-lab) ![Star](https://img.shields.io/github/stars/rehg-lab/RAVE.svg?style=social&label=Star)
- **MagicStick: Controllable Video Editing via Control Handle
  Transformations**, `arXiv, 2312.03047`, [arxiv](http://arxiv.org/abs/2312.03047v1), [pdf](http://arxiv.org/pdf/2312.03047v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14864055399360135809&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen*
- **DragVideo: Interactive Drag-style Video Editing**, `arXiv, 2312.02216`, [arxiv](http://arxiv.org/abs/2312.02216v1), [pdf](http://arxiv.org/pdf/2312.02216v1.pdf), cication: [**-1**](None)

	 *Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, Chi-Keung Tang*
- **Generative Rendering: Controllable 4D-Guided Video Generation with 2D
  Diffusion Models**, `arXiv, 2312.01409`, [arxiv](http://arxiv.org/abs/2312.01409v1), [pdf](http://arxiv.org/pdf/2312.01409v1.pdf), cication: [**-1**](None)

	 *Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Paul Huang, Tuanfeng Yang Wang, Gordon Wetzstein*
- **VideoSwap: Customized Video Subject Swapping with Interactive Semantic
  Point Correspondence**, `arXiv, 2312.02087`, [arxiv](http://arxiv.org/abs/2312.02087v2), [pdf](http://arxiv.org/pdf/2312.02087v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2760288141036862602&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang*
- **Sketch Video Synthesis**, `arXiv, 2311.15306`, [arxiv](http://arxiv.org/abs/2311.15306v1), [pdf](http://arxiv.org/pdf/2311.15306v1.pdf), cication: [**-1**](None)

	 *Yudian Zheng, Xiaodong Cun, Menghan Xia, Chi-Man Pun* · ([sketchvideo](https://github.com/yudianzheng/sketchvideo) - yudianzheng) ![Star](https://img.shields.io/github/stars/yudianzheng/sketchvideo.svg?style=social&label=Star)
- **Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation**, `arXiv, 2306.07954`, [arxiv](http://arxiv.org/abs/2306.07954v2), [pdf](http://arxiv.org/pdf/2306.07954v2.pdf), cication: [**48**](https://scholar.google.com/scholar?cites=4772856106441552899&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy* · ([Rerender_A_Video](https://github.com/williamyang1991/Rerender_A_Video) - williamyang1991) ![Star](https://img.shields.io/github/stars/williamyang1991/Rerender_A_Video.svg?style=social&label=Star)

## Dection
- **What Matters in Detecting AI-Generated Videos like Sora?**, `arXiv, 2406.19568`, [arxiv](http://arxiv.org/abs/2406.19568v1), [pdf](http://arxiv.org/pdf/2406.19568v1.pdf), cication: [**-1**](None)

	 *Chirui Chang, Zhengzhe Liu, Xiaoyang Lyu, Xiaojuan Qi*

	 · ([justin-crchang.github](https://justin-crchang.github.io/3DCNNDetection.github.io/))
- **DeMamba: AI-Generated Video Detection on Million-Scale GenVideo
  Benchmark**, `arXiv, 2405.19707`, [arxiv](http://arxiv.org/abs/2405.19707v1), [pdf](http://arxiv.org/pdf/2405.19707v1.pdf), cication: [**-1**](None)

	 *Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang* · ([DeMamba](https://github.com/chenhaoxing/DeMamba) - chenhaoxing) ![Star](https://img.shields.io/github/stars/chenhaoxing/DeMamba.svg?style=social&label=Star)

## Datasets
- **MiraData: A Large-Scale Video Dataset with Long Durations and Structured
  Captions**, `arXiv, 2407.06358`, [arxiv](http://arxiv.org/abs/2407.06358v1), [pdf](http://arxiv.org/pdf/2407.06358v1.pdf), cication: [**-1**](None)

	 *Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, Ying Shan*
- **OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video
  Generation**, `arXiv, 2407.02371`, [arxiv](http://arxiv.org/abs/2407.02371v1), [pdf](http://arxiv.org/pdf/2407.02371v1.pdf), cication: [**-1**](None)

	 *Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, Ying Tai*

	 · ([nju-pcalab.github](https://nju-pcalab.github.io/projects/openvid))
- **VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video
  Diffusion Models**, `arXiv, 2403.06098`, [arxiv](http://arxiv.org/abs/2403.06098v3), [pdf](http://arxiv.org/pdf/2403.06098v3.pdf), cication: [**-1**](None)

	 *Wenhao Wang, Yi Yang*
- **VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video
  Diffusion Models**, `arXiv, 2403.06098`, [arxiv](http://arxiv.org/abs/2403.06098v1), [pdf](http://arxiv.org/pdf/2403.06098v1.pdf), cication: [**-1**](None)

	 *Wenhao Wang, Yi Yang*

## Evaluation
- **Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing
  Reliability,Reproducibility, and Practicality**, `arXiv, 2406.08845`, [arxiv](http://arxiv.org/abs/2406.08845v1), [pdf](http://arxiv.org/pdf/2406.08845v1.pdf), cication: [**-1**](None)

	 *Tianle Zhang, Langtian Ma, Yuchen Yan, Yuchen Zhang, Kai Wang, Yue Yang, Ziyao Guo, Wenqi Shao, Yang You, Yu Qiao*
- **TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and
  Image-to-Video Generation**, `arXiv, 2406.08656`, [arxiv](http://arxiv.org/abs/2406.08656v1), [pdf](http://arxiv.org/pdf/2406.08656v1.pdf), cication: [**-1**](None)

	 *Weixi Feng, Jiachen Li, Michael Saxon, Tsu-jui Fu, Wenhu Chen, William Yang Wang*
- **MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation
  in Videos**, `arXiv, 2406.08407`, [arxiv](http://arxiv.org/abs/2406.08407v1), [pdf](http://arxiv.org/pdf/2406.08407v1.pdf), cication: [**-1**](None)

	 *Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang*
- **VBench: Comprehensive Benchmark Suite for Video Generative Models**, `arXiv, 2311.17982`, [arxiv](http://arxiv.org/abs/2311.17982v1), [pdf](http://arxiv.org/pdf/2311.17982v1.pdf), cication: [**-1**](None)

	 *Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit* · ([VBench](https://github.com/Vchitect/VBench) - Vchitect) ![Star](https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&label=Star)

## Toolkits
- [**SoraWebui**](https://github.com/SoraWebui/SoraWebui?tab=readme-ov-file) - SoraWebui ![Star](https://img.shields.io/github/stars/SoraWebui/SoraWebui.svg?style=social&label=Star)

	 *SoraWebui is an open-source Sora web client, enabling users to easily create videos from text with OpenAI's Sora model.*

## Alignment
- **VideoScore: Building Automatic Metrics to Simulate Fine-grained Human
  Feedback for Video Generation**, `arXiv, 2406.15252`, [arxiv](http://arxiv.org/abs/2406.15252v2), [pdf](http://arxiv.org/pdf/2406.15252v2.pdf), cication: [**-1**](None)

	 *Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj* · ([tiger-ai-lab.github](https://tiger-ai-lab.github.io/VideoScore/))

## Projects
- [**VGen**](https://github.com/ali-vilab/VGen) - ali-vilab ![Star](https://img.shields.io/github/stars/ali-vilab/VGen.svg?style=social&label=Star)

	 *Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models*
- [**MoneyPrinterTurbo**](https://github.com/harry0703/MoneyPrinterTurbo) - harry0703 ![Star](https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=social&label=Star)

	 *利用大模型，一键生成短视频*

## Other
- [Diffusion Models for Video Generation | Lil'Log](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)
- [Introducing Steerable Motion 1.3 - drive videos with batches of images - now higher detail, smoother motion and with better control!](https://www.reddit.com/r/StableDiffusion/comments/1bzakf3/introducing_steerable_motion_13_drive_videos_with/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-gemini-pro-and-gpt4t-vision-go-ga-on-the)
- [Tim Brooks and Bill Peebles  from the OpenAI Sora team @ AGI House keynote](https://twitter.com/agihouse_org/status/1776827897892024734)

- [再谈复现 Sora：被仰望与被遗忘的 | 机器之心](https://www.jiqizhixin.com/articles/2024-03-27-5)
- [Sora 时代的 AI 视频生成何去何从？ | 机器之心](https://www.jiqizhixin.com/articles/2024-03-07-5)
- [去魅Sora: OpenAI 鲜肉小组的小试牛刀](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495615&idx=1&sn=195542f7c0cb64e0c81fa54a6726bkkkk442)
- [OpenAI文生视频方案Sora技术浅析](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495581&idx=2&sn=12e2f966688e61d81d8265bcd7ed8618kk)
- [AI视频年大爆发！Gen-2/Pika成时代爆款，2023年AI视频生成领域的现状全盘点](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652443771&idx=2&sn=ca19aa545677e812915e7e67e0c3e250)
- [VideoPoet｜LLM带来真正的视觉智能](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495446&idx=1&sn=4089da3bd1dd316cf56b65ab693d5677&poc_token=HE85t2WjjLn1_BtacLx6xHH-RLg8xAVsmHpv96WR)
- [AI 视频生成距「GPT时刻」还有多远？](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495252&idx=1&sn=db40cdaf621b5dbe9d564274426ad26d)

## Product
- [Introducing Gen-3 Alpha: A New Frontier for Video Generation](https://runwayml.com/blog/introducing-gen-3-alpha/)
- [Fetching Title#ar69](https://lumalabs.ai/dream-machine)
- [AI startup Higgsfield unveiled a new AI video model called NOVA-1.](https://x.com/adcock_brett/status/1797298164753469522)
- [Krea AI launched Krea Video. ](https://twitter.com/adcock_brett/status/1789687915116654678)
- [VIGGLE](https://viggle.ai/)
- [Higgsfield.ai](https://higgsfield.ai/Blog/we-ve-raised-8m-in-seed-funding-to-unlock-personalized-ai-video-creation-and-creativity)
- [Storytelling Transformed | LTX Studio](https://ltx.studio/)
- [PixVerse - Create breath-taking videos with PixVerse AI](https://pixverse.ai/)

- [清华系多模态大模型公司刚刚融了数亿元！放话"今年达到Sora效果" | 量子位](https://www.qbitai.com/2024/03/127279.html)
- [国产Sora来了，4K 60帧15秒视频刷新纪录！500亿美元短剧出海市场被撬动](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652454101&idx=1&sn=a317255d4989a37a91000c81945792d6)
- [Sora 竞争对手融资数千万美元，目前可免费“薅” | 机器之心](https://www.jiqizhixin.com/articles/2024-03-08-7)