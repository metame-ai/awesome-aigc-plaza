# Awesome Video Generation

- [Awesome Video Generation](#awesome-video-generation)
	- [Video Generation](#video-generation)
	- [Animation](#animation)
	- [Video Editting](#video-editting)
	- [Other](#other)


## Video Generation
- [[2402.00769] AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning](https://arxiv.org/abs/2402.00769)
	- [GitHub - G-U-N/AnimateLCM: AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning](https://github.com/G-U-N/AnimateLCM)
- [Fetching Title#yy3n](https://arxiv.org/abs/2401.09047)
	- [GitHub - AILab-CVC/VideoCrafter: VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](https://github.com/AILab-CVC/VideoCrafter)
- [[2401.12945] Lumiere: A Space-Time Diffusion Model for Video Generation](https://arxiv.org/abs/2401.12945)
- [[2401.10404] Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution](https://arxiv.org/abs/2401.10404)
- [[2401.09962] CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects](https://arxiv.org/abs/2401.09962)
- [[2401.09985] WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens](https://arxiv.org/abs/2401.09985)
- [[2401.09047] VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](https://arxiv.org/abs/2401.09047)
	- [GitHub - AILab-CVC/VideoCrafter: VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](https://github.com/AILab-CVC/VideoCrafter)
	- [VideoCrafter2](https://ailab-cvc.github.io/videocrafter2)
	- [VideoCrafter Demo - a Hugging Face Space by VideoCrafter](https://huggingface.co/spaces/VideoCrafter/VideoCrafter2)
- [Fetching Title#7jby](https://arxiv.org/abs/2401.09084)
- [[2401.09047] VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](https://arxiv.org/abs/2401.09047)
	- [GitHub - AILab-CVC/VideoCrafter: VideoCrafter1: Open Diffusion Models for High-Quality Video Generation](https://github.com/AILab-CVC/VideoCrafter)
	- [Site Unreachable](https://ailab-cvc.github.io/videocrafter;)
- [Site Unreachable](https://arxiv.org/abs/2401.09414)
	- [GitHub - zhuangshaobin/Vlogger: Make Your Dream A Vlog](https://github.com/zhuangshaobin/Vlogger)
- [Site Unreachable](https://arxiv.org/abs/2401.09084)
	- [Site Unreachable](https://univg-baidu.github.io/)
- [Site Unreachable](https://arxiv.org/abs/2401.07781)
- [[2401.03048v1] Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/abs/2401.03048)
	- [Latte: Latent Diffusion Transformer for Video Generation](https://maxin-cn.github.io/latte_project/)
	- [GitHub - maxin-cn/Latte: The official implementation of Latte: Latent Diffusion Transformer for Video Generation.](https://github.com/maxin-cn/Latte)
- [[2401.04468] MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation](https://arxiv.org/abs/2401.04468)
	- [MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation](https://magicvideov2.github.io/)
- [[2401.01651] AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI](https://arxiv.org/abs/2401.01651)
- [[2401.01827] Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions](https://arxiv.org/abs/2401.01827)
	- [GitHub - salesforce/LAVIS: LAVIS - A One-stop Library for Language-Vision Intelligence](https://github.com/salesforce/LAVIS)
- [[2401.01256] VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM](https://arxiv.org/abs/2401.01256)
- [[2401.00896] TrailBlazer: Trajectory Control for Diffusion-Based Video Generation](https://arxiv.org/abs/2401.00896)
- [[2312.16693] I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models](https://arxiv.org/abs/2312.16693)
- [[2312.15770] A Recipe for Scaling up Text-to-Video Generation with Text-free Videos](https://arxiv.org/abs/2312.15770)
	- [TF-T2V](https://tf-t2v.github.io/)
- [[2312.03641] MotionCtrl: A Unified and Flexible Motion Controller for Video Generation](https://arxiv.org/abs/2312.03641)
	- [GitHub - TencentARC/MotionCtrl](https://github.com/TencentARC/MotionCtrl)
- [[2312.14385] Generative AI Beyond LLMs: System Implications of Multi-Modal Generation](https://arxiv.org/abs/2312.14385)
- [[2312.12490] InstructVideo: Instructing Video Diffusion Models with Human Feedback](https://arxiv.org/abs/2312.12490)
- [[2312.14125] VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://arxiv.org/abs/2312.14125)
	- [VideoPoet: A large language model for zero-shot video generation – Google Research Blog](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html)
- [[2312.10899] MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling via Multi-Layered Semantic-Aware Denoising](https://arxiv.org/abs/2312.10899)
- [[2312.09109] VideoLCM: Video Latent Consistency Model](https://arxiv.org/abs/2312.09109)
- [[2312.05107] DreaMoving: A Human Video Generation Framework based on Diffusion Models](https://arxiv.org/abs/2312.05107)
	- [GitHub - dreamoving/dreamoving-project: Official implementation of DreaMoving](https://github.com/dreamoving/dreamoving-project)
- [[2312.07509] PEEKABOO: Interactive Video Generation via Masked-Diffusion](https://arxiv.org/abs/2312.07509)
- [[2312.07537] FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)
	- [GitHub - TianxingWu/FreeInit: FreeInit : Bridging Initialization Gap in Video Diffusion Models](https://github.com/TianxingWu/FreeInit)
	- [FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://tianxingwu.github.io/pages/FreeInit/)
- [[2312.06662] Photorealistic Video Generation with Diffusion Models](https://arxiv.org/abs/2312.06662)
	- [Photorealistic Video Generation with Diffusion Models](https://walt-video-diffusion.github.io/)
	- [W.A.L.T.pdf](https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf)
- [[2312.04966] Customizing Motion in Text-to-Video Diffusion Models](https://arxiv.org/abs/2312.04966)
- [[2312.03018] DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](http://arxiv.org/abs/2312.03018)
- [[2312.00330] StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter](http://arxiv.org/abs/2312.00330)
- [[2312.02813] BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models](http://arxiv.org/abs/2312.02813)
- [[2312.04433] DreamVideo: Composing Your Dream Videos with Customized Subject and Motion](https://arxiv.org/abs/2312.04433)
	- [DreamVideo](https://dreamvideo-t2v.github.io)
- [[2312.04483] Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation](https://arxiv.org/abs/2312.04483)
	- [HiGen](https://higen-t2v.github.io)
- [[2312.04557] GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation](https://arxiv.org/abs/2312.04557)
- [[2312.03641] MotionCtrl: A Unified and Flexible Motion Controller for Video Generation](https://arxiv.org/abs/2312.03641)
- [[2312.02919] Fine-grained Controllable Video Generation via Object Appearance and Context](https://arxiv.org/abs/2312.02919)
- [[2312.00845] VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models](https://arxiv.org/abs/2312.00845)
	- [GitHub - HyeonHo99/Video-Motion-Customization: This repository is the official implementation of "VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models"](https://github.com/HyeonHo99/Video-Motion-Customization)
	- [VMC](https://video-motion-customization.github.io/)
- [[2312.00777] VideoBooth: Diffusion-based Video Generation with Image Prompts](https://arxiv.org/abs/2312.00777)
- [[2311.18829] MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation](http://arxiv.org/abs/2311.18829)
- [[2311.04145] I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models](https://arxiv.org/abs/2311.04145)
	- [I2VGen-XL](https://i2vgen-xl.github.io/)
	- [I2VGen-XL - a Hugging Face Space by damo-vilab](https://huggingface.co/spaces/damo-vilab/I2VGen-XL)
	- [GitHub - damo-vilab/i2vgen-xl: Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models](https://github.com/damo-vilab/i2vgen-xl)
- [[2311.13073] FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline](https://arxiv.org/abs/2311.13073)
	- [Kandinsky Video](https://ai-forever.github.io/kandinsky-video/)
	- [GitHub - ai-forever/KandinskyVideo: KandinskyVideo — multilingual end-to-end text2video latent diffusion model](https://github.com/ai-forever/kandinskyvideo)
- [Stable Video Diffusion ](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)
	- [stabilityai/stable-video-diffusion-img2vid · Hugging Face](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid)
	- [GitHub - Stability-AI/generative-models: Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)
	- [Stable Video Diffusion - a Hugging Face Space by multimodalart](https://huggingface.co/spaces/multimodalart/stable-video-diffusion)
- [[2311.11325] MoVideo: Motion-Aware Video Generation with Diffusion Models](https://arxiv.org/abs/2311.11325)
	- [MoVideo: Motion-Aware Video Generation with Diffusion Models](https://jingyunliang.github.io/MoVideo/)
- [[2311.10709] Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning](https://arxiv.org/abs/2311.10709)
	- [Emu Video | Meta](https://emu-video.metademolab.com/)
	- [https://emu-video.metademolab.com/assets/emu\_video.pdf](https://emu-video.metademolab.com/assets/emu_video.pdf)
- [[2311.01813] FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation](https://arxiv.org/abs/2311.01813)
	- [GitHub - llyx97/FETV: [NeurIPS 2023 Datasets and Benchmarks] "FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation", Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, Lu Hou](https://github.com/llyx97/FETV)
- [[2310.15169] FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling](https://arxiv.org/abs/2310.15169)
	- [汤晓鸥弟子带队：免调优长视频生成，可支持512帧！任何扩散模型都能用｜ICLR'24 | 量子位](https://www.qbitai.com/2024/01/116406.html)
- [[2310.08465] MotionDirector: Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2310.08465)
	- [GitHub - showlab/MotionDirector: MotionDirector: Motion Customization of Text-to-Video Diffusion Models.](https://github.com/showlab/MotionDirector)
- [[2309.15103] LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models](https://arxiv.org/abs/2309.15103)
	- [GitHub - Vchitect/LaVie: LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models](https://github.com/Vchitect/LaVie)

## Animation
- [[2401.15977] Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling](https://arxiv.org/abs/2401.15977)
- [[2401.13363] Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons](https://arxiv.org/abs/2401.13363)
- [[2401.10889] Synthesizing Moving People with 3D Control](https://arxiv.org/abs/2401.10889)
	- [Synthesizing Moving People with 3D Control](https://boyiliee.github.io/3DHM.github.io/)
- [[2401.09146] Continuous Piecewise-Affine Based Motion Model for Image Animation](https://arxiv.org/abs/2401.09146)
- [Motionshop-Replace the characters in video with 3D avatars](https://aigc3d.github.io/motionshop/)
- [GitHub - MooreThreads/Moore-AnimateAnyone](https://github.com/MooreThreads/Moore-AnimateAnyone?tab=readme-ov-file#-gradio-demo)
- [GitHub - Lightricks/LongAnimateDiff](https://github.com/Lightricks/LongAnimateDiff)
	- [LongAnimateDiff - a Hugging Face Space by Lightricks](https://huggingface.co/spaces/Lightricks/LongAnimateDiff)
- [[2312.03884] WonderJourney: Going from Anywhere to Everywhere](https://arxiv.org/abs/2312.03884)
	- [GitHub - KovenYu/WonderJourney](https://github.com/KovenYu/WonderJourney)
	- [WonderJourney](https://kovenyu.com/wonderjourney/)
	- [link](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652418572&idx=4&sn=8e325065ee30dd09cefef21ced19e3b7)
- [[2311.14603] Animate124: Animating One Image to 4D Dynamic Scene](https://arxiv.org/abs/2311.14603)
	- [GitHub - HeliosZhao/Animate124: Animate124: Animating One Image to 4D Dynamic Scene](https://github.com/HeliosZhao/Animate124)
	- [Animate124: Animating One Image to 4D Dynamic Scene](https://animate124.github.io/)
- [[2310.12190] DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://arxiv.org/abs/2310.12190)
	- [DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://doubiiu.github.io/projects/DynamiCrafter/)
	- [GitHub - Doubiiu/DynamiCrafter: DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://github.com/Doubiiu/DynamiCrafter)
	- [DynamiCrafter - a Hugging Face Space by Doubiiu](https://huggingface.co/spaces/Doubiiu/DynamiCrafter)
- [Fetching Title#r7it](https://arxiv.org/abs/2312.03793)
	- [Fetching Title#8vkd](https://vvictoryuki.github.io/animatezero.github.io/)
- [[2312.02928] LivePhoto: Real Image Animation with Text-guided Motion Control](https://arxiv.org/abs/2312.02928)
	- [GitHub - XavierCHEN34/LivePhoto](https://github.com/XavierCHEN34/LivePhoto)
	- [LivePhoto](https://xavierchen34.github.io/LivePhoto-Page/)
- [[2311.16498] MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://arxiv.org/abs/2311.16498)
	- [GitHub - magic-research/magic-animate: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://github.com/magic-research/magic-animate)
	- [MagicAnimate - a Hugging Face Space by zcxu-eric](https://huggingface.co/spaces/zcxu-eric/magicanimate)
	- [一张照片，TikTok小姐姐就都能跳舞了 | 机器之心](https://www.jiqizhixin.com/articles/2023-12-05-4)
	- [GitHub - sdbds/magic-animate-for-windows: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://github.com/sdbds/magic-animate-for-windows)
- [[2311.17117] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117)
	- [GitHub - HumanAIGC/AnimateAnyone: Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://github.com/HumanAIGC/AnimateAnyone)
	- [GitHub - guoqincode/AnimateAnyone-unofficial: Unofficial Implementation of Animate Anyone](https://github.com/guoqincode/AnimateAnyone-unofficial)
- [[2310.20700] SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction](https://arxiv.org/abs/2310.20700)
	- [GitHub - Vchitect/SEINE: SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction](https://github.com/Vchitect/SEINE)
	- [Seine - a Hugging Face Space by Vchitect](https://huggingface.co/spaces/Vchitect/SEINE)
- [[2311.12052] MagicDance: Realistic Human Dance Video Generation with Motions & Facial Expressions Transfer](https://arxiv.org/abs/2311.12052)
- [[2311.10982] Make Pixels Dance: High-Dynamic Video Generation](https://arxiv.org/abs/2311.10982)
	- [Make Pixels Dance: High-Dynamic Video Generation](https://makepixelsdance.github.io/)
- [[2308.08089] DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory](https://arxiv.org/abs/2308.08089)
	- [GitHub - ProjectNUWA/DragNUWA](https://github.com/ProjectNUWA/DragNUWA)
	- [DragNUWA - a Hugging Face Space by yinsming](https://huggingface.co/spaces/yinsming/DragNUWA)
- [[2307.04725] AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)
	- [GitHub - guoyww/AnimateDiff: Official implementation of AnimateDiff.](https://github.com/guoyww/AnimateDiff)
- [[2304.01186] Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos](https://arxiv.org/abs/2304.01186)
	- [GitHub - mayuelala/FollowYourPose: [AAAI 2024] Follow-Your-Pose: This repo is the official implementation of "Follow-Your-Pose : Pose-Guided Text-to-Video Generation using Pose-Free Videos"](https://github.com/mayuelala/FollowYourPose)

## Video Editting
- [[2401.17509] Anything in Any Scene: Photorealistic Video Object Insertion](https://arxiv.org/abs/2401.17509)
	- [Anything in Any Scene: Photorealistic Video Object Insertion](https://anythinginanyscene.github.io)
- [[2401.10822] ActAnywhere: Subject-Aware Video Background Generation](https://arxiv.org/abs/2401.10822)
	- [ActAnywhere](https://actanywhere.github.io/)
- [[2401.05735] Object-Centric Diffusion for Efficient Video Editing](https://arxiv.org/abs/2401.05735)
- [[2312.17681] FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://arxiv.org/abs/2312.17681)
	- [FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://jeff-liangf.github.io/projects/flowvid/)
- [[2312.13834] Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis](https://arxiv.org/abs/2312.13834)
- [[2312.12468] MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers](https://arxiv.org/abs/2312.12468)
- [[2312.10656] VidToMe: Video Token Merging for Zero-Shot Video Editing](https://arxiv.org/abs/2312.10656)
- [[2312.04524] RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models](https://arxiv.org/abs/2312.04524)
	- [GitHub - rehg-lab/RAVE: RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models - Official Repo](https://github.com/rehg-lab/RAVE)
- [[2312.03047] MagicStick: Controllable Video Editing via Control Handle Transformations](https://arxiv.org/abs/2312.03047)
- [[2312.02216] DragVideo: Interactive Drag-style Video Editing](https://arxiv.org/abs/2312.02216)
- [[2312.01409] Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models](https://arxiv.org/abs/2312.01409)
- [[2312.02087] VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence](https://arxiv.org/abs/2312.02087)
- [[2311.15306] Sketch Video Synthesis](https://arxiv.org/abs/2311.15306)
	- [GitHub - yudianzheng/SketchVideo: [arXiv 2023] Sketch Video Synthesis](https://github.com/yudianzheng/sketchvideo)
- [[2306.07954] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation](https://arxiv.org/abs/2306.07954)
	- [GitHub - williamyang1991/Rerender\_A\_Video: [SIGGRAPH Asia 2023] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation](https://github.com/williamyang1991/Rerender_A_Video)
## Other
- [VideoPoet｜LLM带来真正的视觉智能](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495446&idx=1&sn=4089da3bd1dd316cf56b65ab693d5677&poc_token=HE85t2WjjLn1_BtacLx6xHH-RLg8xAVsmHpv96WR)
- [AI 视频生成距「GPT时刻」还有多远？](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495252&idx=1&sn=db40cdaf621b5dbe9d564274426ad26d)